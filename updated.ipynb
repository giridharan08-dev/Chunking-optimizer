{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7c3fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, uuid, pathlib  #step 1\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as pacsv\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# ---- Paths ----\n",
    "DATA_CSV = \"synthetic_retail_data.csv\"          # change to your real CSV if different\n",
    "CHROMA_PATH = \"chromadb_store\"                  # persistent store folder\n",
    "COLLECTION_NAME = \"retail_chunks_arrow\"\n",
    "JSON_INDEX_PATH = \"arrow_index.json\"\n",
    "\n",
    "# If you want to regenerate synthetic data quickly (set to True to generate)\n",
    "GENERATE_SYNTHETIC = False\n",
    "N_ROWS_SYN = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e70d044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: InvoiceNo: string\n",
      "StockCode: string\n",
      "Description: string\n",
      "Quantity: int64\n",
      "InvoiceDate: date32[day]\n",
      "UnitPrice: double\n",
      "CustomerID: int64\n",
      "Country: string\n",
      "Rows: 200\n",
      "Categorical (chunk text): ['InvoiceNo', 'StockCode', 'Description', 'InvoiceDate', 'Country']\n",
      "Numeric (metadata): ['Quantity', 'UnitPrice', 'CustomerID']\n"
     ]
    }
   ],
   "source": [
    "# Read CSV using PyArrow (fast and type-aware) #step2\n",
    "read_options = pacsv.ReadOptions(autogenerate_column_names=False)\n",
    "parse_options = pacsv.ParseOptions(delimiter=\",\")\n",
    "convert_options = pacsv.ConvertOptions()  # we’ll inspect schema after load\n",
    "\n",
    "table: pa.Table = pacsv.read_csv(DATA_CSV, read_options=read_options,\n",
    "                                 parse_options=parse_options,\n",
    "                                 convert_options=convert_options)\n",
    "\n",
    "print(\"Schema:\", table.schema)\n",
    "print(\"Rows:\", table.num_rows)\n",
    "\n",
    "# ---- Assign roles ----\n",
    "# Treat text-ish columns as \"categorical\" (go into the chunk),\n",
    "# and numeric columns as \"metadata\".\n",
    "# You can modify these lists to fit your real CSV.\n",
    "\n",
    "all_cols = table.schema.names\n",
    "# Heuristic: numeric arrow types\n",
    "numeric_types = (pa.int8(), pa.int16(), pa.int32(), pa.int64(),\n",
    "                 pa.uint8(), pa.uint16(), pa.uint32(), pa.uint64(),\n",
    "                 pa.float16(), pa.float32(), pa.float64())\n",
    "\n",
    "numeric_cols = [c for c in all_cols if pa.types.is_integer(table.schema.field(c).type) or pa.types.is_floating(table.schema.field(c).type)]\n",
    "# Everything else considered categorical (including strings/timestamps)\n",
    "categorical_cols = [c for c in all_cols if c not in numeric_cols]\n",
    "\n",
    "# If you want a specific split (recommended for your dataset), override:\n",
    "preferred_categorical = [\"InvoiceNo\", \"StockCode\", \"Description\", \"InvoiceDate\", \"Country\"]\n",
    "preferred_numeric = [\"Quantity\", \"UnitPrice\", \"CustomerID\"]\n",
    "\n",
    "# Use preferred if they exist; fallback to detected\n",
    "categorical_cols = [c for c in preferred_categorical if c in all_cols] or categorical_cols\n",
    "numeric_cols = [c for c in preferred_numeric if c in all_cols] or numeric_cols\n",
    "\n",
    "print(\"Categorical (chunk text):\", categorical_cols)\n",
    "print(\"Numeric (metadata):\", numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0128eeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 200 chunks. Example:\n",
      " ['InvoiceNo: INV1000, StockCode: STK588, Description: Shoes, InvoiceDate: 2023-01-01, Country: Germany', 'InvoiceNo: INV1001, StockCode: STK335, Description: Laptop, InvoiceDate: 2023-01-02, Country: USA'] \n",
      "Metadata example:\n",
      " [{'Quantity': 3, 'UnitPrice': 306.04, 'CustomerID': 11745, 'Country': 'Germany', 'Description': 'Shoes', 'InvoiceDate': '2023-01-01', 'InvoiceNo': 'INV1000', 'StockCode': 'STK588'}, {'Quantity': 10, 'UnitPrice': 63.06, 'CustomerID': 12621, 'Country': 'USA', 'Description': 'Laptop', 'InvoiceDate': '2023-01-02', 'InvoiceNo': 'INV1001', 'StockCode': 'STK335'}]\n"
     ]
    }
   ],
   "source": [
    "def to_python_value(arr, i):\n",
    "    # Convert a scalar at row i in a PyArrow array to a Python type\n",
    "    return arr[i].as_py()\n",
    "\n",
    "def build_row_dict(batch, row_idx_in_batch):\n",
    "    out = {}\n",
    "    for col_name in batch.schema.names:\n",
    "        out[col_name] = to_python_value(batch.column(col_name), row_idx_in_batch)\n",
    "    return out\n",
    "\n",
    "def make_chunk_text(row_dict, cat_cols):\n",
    "    # Compose only categorical fields into the text chunk\n",
    "    parts = [f\"{c}: {row_dict.get(c, '')}\" for c in cat_cols]\n",
    "    return \", \".join(parts)\n",
    "\n",
    "chunks = []\n",
    "metadatas = []\n",
    "id_list = []\n",
    "row_counter = 0\n",
    "\n",
    "for batch in table.to_batches(max_chunksize=2048):\n",
    "    batch = pa.Table.from_batches([batch])  # ensure schema consistency\n",
    "    n = batch.num_rows\n",
    "    for i in range(n):\n",
    "        row = build_row_dict(batch, i)\n",
    "\n",
    "        # Build chunk text from categorical fields\n",
    "        text = make_chunk_text(row, categorical_cols)\n",
    "\n",
    "        # Metadata: numeric fields\n",
    "        meta = {k: row[k] for k in numeric_cols}\n",
    "\n",
    "        # Keep a few categorical keys as metadata (converted safely)\n",
    "        for keep in [\"Country\", \"Description\", \"InvoiceDate\", \"InvoiceNo\", \"StockCode\"]:\n",
    "            if keep in row:\n",
    "                val = row[keep]\n",
    "                # Convert unsupported types (dates/timestamps) → string\n",
    "                if hasattr(val, \"isoformat\"):\n",
    "                    val = str(val)\n",
    "                elif val is None:\n",
    "                    val = None\n",
    "                else:\n",
    "                    # Non-numeric categorical values → string\n",
    "                    val = str(val) if not isinstance(val, (int, float, bool)) else val\n",
    "                meta[keep] = val\n",
    "\n",
    "        # Create a stable ID\n",
    "        _id = str(row_counter)\n",
    "\n",
    "        id_list.append(_id)\n",
    "        chunks.append(text)\n",
    "        metadatas.append(meta)\n",
    "        row_counter += 1\n",
    "\n",
    "print(f\"Built {len(chunks)} chunks. Example:\\n\", chunks[:2], \"\\nMetadata example:\\n\", metadatas[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afe6946e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02474d7000b94b569ec6987d88e81609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: 200 x 384\n",
      "✅ Stored in ChromaDB: retail_chunks_arrow Count: 200\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model (fast & small)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode in batches\n",
    "embeddings = model.encode(chunks, batch_size=128, show_progress_bar=True)\n",
    "print(\"Embeddings shape:\", len(embeddings), \"x\", len(embeddings[0]))\n",
    "\n",
    "# Connect to persistent ChromaDB\n",
    "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = client.get_or_create_collection(COLLECTION_NAME)\n",
    "\n",
    "# Optional: clear previous data safely (avoid where={} errors)\n",
    "if collection.count() > 0:\n",
    "    existing = collection.get()\n",
    "    all_ids = existing.get(\"ids\", [])\n",
    "    if all_ids:\n",
    "        collection.delete(ids=all_ids)\n",
    "\n",
    "# Add in manageable batches to avoid RAM spikes\n",
    "BATCH = 1000\n",
    "for start in range(0, len(chunks), BATCH):\n",
    "    end = start + BATCH\n",
    "    collection.add(\n",
    "        ids=id_list[start:end],\n",
    "        documents=chunks[start:end],\n",
    "        embeddings=embeddings[start:end],\n",
    "        metadatas=metadatas[start:end]\n",
    "    )\n",
    "\n",
    "print(\"✅ Stored in ChromaDB:\", COLLECTION_NAME, \"Count:\", collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f51a204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote JSON index → arrow_index.json\n"
     ]
    }
   ],
   "source": [
    "# Build a fast lookup index\n",
    "index = {\n",
    "    \"version\": \"1.0\",\n",
    "    \"source_csv\": str(DATA_CSV),\n",
    "    \"collection\": COLLECTION_NAME,\n",
    "    \"field_roles\": {\n",
    "        \"categorical\": categorical_cols,\n",
    "        \"numeric_meta\": numeric_cols\n",
    "    },\n",
    "    \"id_mapping\": {},  # id -> minimal pointer info\n",
    "    \"stats\": {\n",
    "        \"rows\": len(id_list),\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"embedding_dim\": int(len(embeddings[0])) if len(embeddings) else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Minimal mapping: id to a subset you might want to fetch quickly without reading Chroma\n",
    "# We’ll include a primary key if available (InvoiceNo) and a couple of common fields.\n",
    "for _id, meta in zip(id_list, metadatas):\n",
    "    index[\"id_mapping\"][_id] = {\n",
    "        \"InvoiceNo\": meta.get(\"InvoiceNo\"),\n",
    "        \"Description\": meta.get(\"Description\"),\n",
    "        \"Country\": meta.get(\"Country\")\n",
    "    }\n",
    "\n",
    "with open(JSON_INDEX_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(index, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Wrote JSON index → {JSON_INDEX_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c3706ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(distance: 1.2907)\n",
      "Doc: InvoiceNo: INV1077, StockCode: STK462, Description: Laptop, InvoiceDate: 2023-03-19, Country: India\n",
      "Meta: {'UnitPrice': 157.66, 'StockCode': 'STK462', 'Quantity': 5, 'InvoiceDate': '2023-03-19', 'Country': 'India', 'CustomerID': 11308, 'InvoiceNo': 'INV1077', 'Description': 'Laptop'}\n",
      "\n",
      "(distance: 1.3074)\n",
      "Doc: InvoiceNo: INV1062, StockCode: STK749, Description: Laptop, InvoiceDate: 2023-03-04, Country: India\n",
      "Meta: {'CustomerID': 15547, 'StockCode': 'STK749', 'Description': 'Laptop', 'Quantity': 5, 'InvoiceDate': '2023-03-04', 'UnitPrice': 157.69, 'InvoiceNo': 'INV1062', 'Country': 'India'}\n",
      "\n",
      "(distance: 1.3128)\n",
      "Doc: InvoiceNo: INV1008, StockCode: STK147, Description: Laptop, InvoiceDate: 2023-01-09, Country: Germany\n",
      "Meta: {'CustomerID': 14914, 'StockCode': 'STK147', 'InvoiceNo': 'INV1008', 'Description': 'Laptop', 'InvoiceDate': '2023-01-09', 'Quantity': 1, 'UnitPrice': 389.69, 'Country': 'Germany'}\n",
      "\n",
      "(distance: 1.3145)\n",
      "Doc: InvoiceNo: INV1196, StockCode: STK952, Description: Laptop, InvoiceDate: 2023-07-16, Country: UK\n",
      "Meta: {'UnitPrice': 471.52, 'StockCode': 'STK952', 'InvoiceDate': '2023-07-16', 'Country': 'UK', 'InvoiceNo': 'INV1196', 'Quantity': 5, 'Description': 'Laptop', 'CustomerID': 13237}\n",
      "\n",
      "(distance: 1.3179)\n",
      "Doc: InvoiceNo: INV1127, StockCode: STK696, Description: Laptop, InvoiceDate: 2023-05-08, Country: UK\n",
      "Meta: {'InvoiceDate': '2023-05-08', 'InvoiceNo': 'INV1127', 'Quantity': 9, 'Description': 'Laptop', 'StockCode': 'STK696', 'Country': 'UK', 'CustomerID': 17641, 'UnitPrice': 47.17}\n",
      "\n",
      "🔎 From JSON index: {'InvoiceNo': 'INV1000', 'Description': 'Shoes', 'Country': 'Germany'}\n"
     ]
    }
   ],
   "source": [
    "def search(query_text, k=5):\n",
    "    q_emb = model.encode([query_text])\n",
    "    res = collection.query(\n",
    "        query_embeddings=q_emb,\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]  # no \"ids\"\n",
    "    )\n",
    "    return res\n",
    "\n",
    "demo = search(\"cheap laptop\", k=5)\n",
    "\n",
    "for doc, meta, dist in zip(demo[\"documents\"][0], demo[\"metadatas\"][0], demo[\"distances\"][0]):\n",
    "    print(f\"\\n(distance: {dist:.4f})\")\n",
    "    print(\"Doc:\", doc)\n",
    "    print(\"Meta:\", meta)\n",
    "\n",
    "# Use JSON index to quickly map back by our own ID list\n",
    "with open(JSON_INDEX_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    jindex = json.load(f)\n",
    "\n",
    "# Example: pick the first returned doc and lookup our JSON index\n",
    "if demo[\"documents\"][0]:\n",
    "    first_doc = demo[\"documents\"][0][0]\n",
    "    # JSON index maps IDs → fields\n",
    "    print(\"\\n🔎 From JSON index:\", jindex[\"id_mapping\"].get(\"0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0887a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query='laptop'  keyword='laptop'  Recall@10=0.36  Hits=['127', '33', '19', '62', '77', '116', '196', '166', '21', '190']  GT_count=28\n",
      "\n",
      "Query='cheap shoes'  keyword='shoes'  Recall@10=0.30  Hits=['92', '55', '199', '78', '152', '173', '142', '90', '144', '167']  GT_count=33\n",
      "\n",
      "Query='book'  keyword='book'  Recall@10=0.32  Hits=['82', '74', '66', '194', '102', '157', '27', '162', '26', '137']  GT_count=31\n"
     ]
    }
   ],
   "source": [
    "# Evaluate recall@k for a few keywords based on Description membership\n",
    "def recall_at_k(query, keyword, k=10):\n",
    "    # Convert PyArrow table → Pandas DataFrame\n",
    "    df_all = table.to_pandas()\n",
    "\n",
    "    # ground truth: rows whose Description contains keyword (case-insensitive)\n",
    "    gt_ids = [str(i) for i, val in enumerate(df_all[\"Description\"].astype(str)) if keyword.lower() in val.lower()]\n",
    "\n",
    "    res = search(query, k=k)\n",
    "    retrieved = set(res[\"ids\"][0]) if \"ids\" in res else set()\n",
    "\n",
    "    if not gt_ids:\n",
    "        return 0.0, [], []\n",
    "\n",
    "    overlap = retrieved & set(gt_ids)\n",
    "    recall = len(overlap) / len(gt_ids)\n",
    "    return recall, list(overlap), gt_ids\n",
    "\n",
    "\n",
    "# Try it\n",
    "for q, kw in [(\"laptop\", \"laptop\"), (\"cheap shoes\", \"shoes\"), (\"book\", \"book\")]:\n",
    "    r, hit, gt = recall_at_k(q, kw, k=10)\n",
    "    print(f\"\\nQuery='{q}'  keyword='{kw}'  Recall@10={r:.2f}  Hits={hit[:10]}  GT_count={len(gt)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
