{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba0b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# optional splitter\n",
    "try:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    LANGCHAIN_AVAILABLE = True\n",
    "except Exception:\n",
    "    LANGCHAIN_AVAILABLE = False\n",
    "\n",
    "# NLTK for stemming/lemmatization/stopwords (used only if requested)\n",
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e57d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "def load_csv(path_or_buffer):\n",
    "    \"\"\"Loads CSV from local path or file-like object. Returns df or raises.\"\"\"\n",
    "    df = pd.read_csv(path_or_buffer)\n",
    "    return df\n",
    "\n",
    "def preview_random(df, n=5):\n",
    "    n = min(n, len(df))\n",
    "    return df.sample(n=n, random_state=42).reset_index(drop=True)\n",
    "\n",
    "def show_dtypes(df):\n",
    "    return pd.DataFrame({\"column\": df.columns, \"dtype\": [str(df[c].dtype) for c in df.columns]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7e1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "def detect_html_columns(df, max_rows=500):\n",
    "    cols = []\n",
    "    sample = df.head(max_rows).astype(str)\n",
    "    for c in sample.columns:\n",
    "        if sample[c].str.contains(r\"<.*?>\", regex=True).any():\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "def detect_missing(df):\n",
    "    missing_pct = df.isna().mean()  # per col\n",
    "    total_missing = df.isna().sum().sum()\n",
    "    return {\"per_column_pct\": missing_pct.to_dict(), \"total_missing\": int(total_missing), \"has_missing\": total_missing>0}\n",
    "\n",
    "def detect_duplicates(df):\n",
    "    dup_count = df.duplicated().sum()\n",
    "    return {\"dup_count\": int(dup_count), \"has_duplicates\": dup_count>0}\n",
    "\n",
    "def detect_multiline(df, max_rows=500):\n",
    "    cols = []\n",
    "    sample = df.head(max_rows).astype(str)\n",
    "    for c in sample.columns:\n",
    "        if sample[c].str.contains(r\"\\n|\\r\", regex=True).any():\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "def detect_delimiter_problem(df):\n",
    "    # heuristic: too many commas inside many fields\n",
    "    sample = df.head(200).astype(str)\n",
    "    count_commas = sample.apply(lambda col: col.str.contains(\",\").sum())\n",
    "    # if any column has > 60% rows containing commas, it's suspicious\n",
    "    return bool((count_commas > 0.6*len(sample)).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a14e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "def to_lowercase(df):\n",
    "    df2 = df.copy()\n",
    "    for c in df2.select_dtypes(include=[\"object\", \"string\"]).columns:\n",
    "        df2[c] = df2[c].astype(str).str.lower()\n",
    "    return df2\n",
    "\n",
    "def remove_html_tags(df, columns=None):\n",
    "    df2 = df.copy()\n",
    "    if columns is None:\n",
    "        columns = detect_html_columns(df2)\n",
    "    for c in columns:\n",
    "        df2[c] = df2[c].astype(str).apply(lambda s: BeautifulSoup(s, \"html.parser\").get_text())\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a90d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def handle_missing(df, action=\"none\", fill_value=\"unknown\"):\n",
    "    df2 = df.copy()\n",
    "    if action == \"fill\":\n",
    "        df2 = df2.fillna(fill_value)\n",
    "    elif action == \"drop\":\n",
    "        df2 = df2.dropna()\n",
    "    return df2\n",
    "\n",
    "def drop_duplicates(df, keep=\"first\"):\n",
    "    return df.drop_duplicates(keep=keep)\n",
    "\n",
    "def fix_delimiters(df):\n",
    "    # try re-parsing CSV text to avoid issues â€” works only with original CSV textual input.\n",
    "    csv_buffer = df.to_csv(index=False)\n",
    "    df2 = pd.read_csv(StringIO(csv_buffer), sep=\",\", engine=\"python\", quotechar='\"')\n",
    "    return df2\n",
    "\n",
    "def fix_multiline_cells(df):\n",
    "    df2 = df.replace(r\"[\\r\\n]+\", \" \", regex=True)\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "645ada7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def normalize_text(s, remove_stopwords=False, stem=False, lemmatize=False):\n",
    "    tokens = word_tokenize(str(s))\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t.lower() not in stop_words]\n",
    "    if lemmatize:\n",
    "        tokens = [wnl.lemmatize(t) for t in tokens]\n",
    "    if stem:\n",
    "        tokens = [ps.stem(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def apply_text_normalization(df, text_columns, remove_stopwords=False, stem=False, lemmatize=False):\n",
    "    df2 = df.copy()\n",
    "    for c in text_columns:\n",
    "        df2[c] = df2[c].astype(str).apply(lambda s: normalize_text(s, remove_stopwords, stem, lemmatize))\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808b1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "def compute_quality_metrics(df, text_cols=None):\n",
    "    metrics = {}\n",
    "    metrics[\"num_rows\"] = len(df)\n",
    "    metrics[\"num_columns\"] = len(df.columns)\n",
    "    miss = detect_missing(df)\n",
    "    metrics[\"total_missing\"] = miss[\"total_missing\"]\n",
    "    dup = detect_duplicates(df)\n",
    "    metrics[\"dup_count\"] = dup[\"dup_count\"]\n",
    "    # tokens per row (estimate)\n",
    "    if text_cols is None:\n",
    "        text_cols = [c for c in df.columns if df[c].dtype == object][:1]  # pick first text col\n",
    "    token_counts = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = \" \".join([str(row[c]) for c in text_cols]) if text_cols else \" \".join(map(str,row.to_list()))\n",
    "        token_counts.append(len(text.split()))\n",
    "    metrics[\"avg_tokens_per_row\"] = float(np.mean(token_counts)) if token_counts else 0.0\n",
    "    return metrics\n",
    "\n",
    "def quality_gate_pass(metrics, thresholds=None):\n",
    "    # thresholds example:\n",
    "    if thresholds is None:\n",
    "        thresholds = {\"max_missing\": 0.2, \"max_dup_pct\": 0.05, \"min_avg_tokens\": 5}\n",
    "    # compute missing fraction\n",
    "    missing_frac = metrics[\"total_missing\"] / max(1, metrics[\"num_rows\"]*metrics[\"num_columns\"])\n",
    "    dup_frac = metrics[\"dup_count\"] / max(1, metrics[\"num_rows\"])\n",
    "    avg_tokens = metrics[\"avg_tokens_per_row\"]\n",
    "    pass_cond = (missing_frac <= thresholds[\"max_missing\"]) and (dup_frac <= thresholds[\"max_dup_pct\"]) and (avg_tokens >= thresholds[\"min_avg_tokens\"])\n",
    "    return pass_cond, {\"missing_frac\": missing_frac, \"dup_frac\": dup_frac, \"avg_tokens\": avg_tokens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6214827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "def document_chunks(df):\n",
    "    docs = []\n",
    "    metas = []\n",
    "    for i,row in df.iterrows():\n",
    "        d = \" | \".join([f\"{c}: {row[c]}\" for c in df.columns])\n",
    "        docs.append(d)\n",
    "        metas.append({c: sanitize_meta(v) for c,v in row.to_dict().items()})\n",
    "    return docs, metas\n",
    "\n",
    "def fixed_row_chunks(df):\n",
    "    docs = []\n",
    "    metas = []\n",
    "    for i,row in df.iterrows():\n",
    "        text = \" | \".join([f\"{c}: {row[c]}\" for c in df.columns])\n",
    "        docs.append(text)\n",
    "        metas.append({c: sanitize_meta(v) for c,v in row.to_dict().items()})\n",
    "    return docs, metas\n",
    "\n",
    "def semantic_chunks(df, text_col):\n",
    "    docs=[]; metas=[]\n",
    "    for i,row in df.iterrows():\n",
    "        text = str(row[text_col])\n",
    "        docs.append(text)\n",
    "        metas.append({c: sanitize_meta(v) for c,v in row.to_dict().items()})\n",
    "    return docs, metas\n",
    "\n",
    "def recursive_chunks(df, chunk_size=400, overlap=50):\n",
    "    docs=[]\n",
    "    metas=[]\n",
    "    texts = [\" \".join(map(str,row.to_list())) for _,row in df.iterrows()]\n",
    "    if LANGCHAIN_AVAILABLE:\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap, separators=[\"\\n\\n\",\"\\n\",\". \",\" \", \"\"])\n",
    "        for i,t in enumerate(texts):\n",
    "            parts = splitter.split_text(t)\n",
    "            for j,p in enumerate(parts):\n",
    "                docs.append(p)\n",
    "                metas.append({\"parent_row\": i, \"child_index\": j})\n",
    "    else:\n",
    "        import re\n",
    "        for i,t in enumerate(texts):\n",
    "            paras = re.split(r\"\\n\\n+\", t)\n",
    "            for j,p in enumerate(paras):\n",
    "                if len(p) <= chunk_size:\n",
    "                    docs.append(p); metas.append({\"parent_row\": i, \"child_index\": j})\n",
    "                else:\n",
    "                    sents = re.split(r'(?<=[.!?])\\s+', p)\n",
    "                    buf=\"\"; idx=0\n",
    "                    for s in sents:\n",
    "                        if len(buf)+len(s) <= chunk_size:\n",
    "                            buf = (buf + \" \" + s).strip()\n",
    "                        else:\n",
    "                            if buf:\n",
    "                                docs.append(buf); metas.append({\"parent_row\": i, \"child_index\": idx}); idx+=1\n",
    "                            buf = s\n",
    "                    if buf:\n",
    "                        docs.append(buf); metas.append({\"parent_row\": i, \"child_index\": idx})\n",
    "    return docs, metas\n",
    "\n",
    "def sanitize_meta(v):\n",
    "    if v is None: return None\n",
    "    if isinstance(v, (str,int,float,bool)): return v\n",
    "    try:\n",
    "        if hasattr(v, \"isoformat\"):\n",
    "            return str(v)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e5feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "def embed_texts(model_name, texts, batch_size=128):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embs = model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "    return embs\n",
    "\n",
    "def store_to_chroma(collection_prefix, model_name, ids, documents, embeddings, metadatas, chroma_path=\"chromadb_store\"):\n",
    "    client = chromadb.PersistentClient(path=chroma_path)\n",
    "    cname = f\"{collection_prefix}_{model_name.replace('/','_').replace(':','_')}\"\n",
    "    try:\n",
    "        col = client.get_collection(cname)\n",
    "    except Exception:\n",
    "        col = client.create_collection(cname)\n",
    "    # optional remove previous\n",
    "    if col.count() > 0:\n",
    "        try:\n",
    "            ex = col.get()\n",
    "            if \"ids\" in ex and ex[\"ids\"]:\n",
    "                col.delete(ids=ex[\"ids\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    # add in batches\n",
    "    B = 500\n",
    "    N = len(ids)\n",
    "    for s in range(0,N,B):\n",
    "        e = min(s+B,N)\n",
    "        col.add(ids=ids[s:e], documents=documents[s:e], embeddings=embeddings[s:e], metadatas=metadatas[s:e])\n",
    "    return col\n",
    "\n",
    "def retrieve_from_chroma(collection_prefix, model_name, query, k=5, chroma_path=\"chromadb_store\", distance_threshold=0.6):\n",
    "    client = chromadb.PersistentClient(path=chroma_path)\n",
    "    cname = f\"{collection_prefix}_{model_name.replace('/','_').replace(':','_')}\"\n",
    "    try:\n",
    "        col = client.get_collection(cname)\n",
    "    except Exception:\n",
    "        return {\"found\": False, \"reason\": \"collection_not_found\", \"results\": []}\n",
    "    model = SentenceTransformer(model_name)\n",
    "    q_emb = model.encode([query])\n",
    "    res = col.query(query_embeddings=q_emb, n_results=k, include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    docs = res.get(\"documents\", [[]])[0]\n",
    "    metas = res.get(\"metadatas\", [[]])[0]\n",
    "    dists = res.get(\"distances\", [[]])[0]\n",
    "    if not docs:\n",
    "        return {\"found\": False, \"reason\": \"no_results\", \"results\": []}\n",
    "    if dists and min(dists) > distance_threshold:\n",
    "        return {\"found\": False, \"reason\": \"too_far\", \"min_distance\": min(dists), \"results\": []}\n",
    "    out = [{\"doc\":d,\"meta\":m,\"dist\":float(dist)} for d,m,dist in zip(docs,metas,dists)]\n",
    "    return {\"found\": True, \"results\": out}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb8c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53af2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "def preview_random(df, n=5):\n",
    "    return df.sample(n) if len(df) > n else df\n",
    "\n",
    "def to_lowercase(df):\n",
    "    return df.applymap(lambda s: s.lower() if isinstance(s, str) else s)\n",
    "\n",
    "def remove_html_tags(df, columns=None):\n",
    "    html_pattern = re.compile(r'<.*?>')\n",
    "    if columns:\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).apply(lambda x: re.sub(html_pattern, '', x))\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame, strategy: str, custom_value: Any = None) -> pd.DataFrame:\n",
    "    if strategy == 'drop':\n",
    "        return df.dropna()\n",
    "    elif strategy == 'fill':\n",
    "        return df.fillna(custom_value)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def process_text(df: pd.DataFrame, text_columns: list, stemming: bool, lemmatization: bool, stopword_removal: bool) -> pd.DataFrame:\n",
    "    return df\n",
    "\n",
    "def quality_gate(df: pd.DataFrame) -> bool:\n",
    "    return df.isnull().sum().sum() == 0\n",
    "\n",
    "def apply_chunking(df: pd.DataFrame, strategy: str, chunk_size: int = None, overlap: int = 0) -> list:\n",
    "    chunks = []\n",
    "    text_data = df.to_string(index=False)\n",
    "    if strategy == 'fixed':\n",
    "        for i in range(0, len(text_data), chunk_size):\n",
    "            chunks.append(text_data[i:i + chunk_size])\n",
    "    elif strategy == 'recursive':\n",
    "        i = 0\n",
    "        while i < len(text_data):\n",
    "            chunk = text_data[i:i + chunk_size]\n",
    "            chunks.append(chunk)\n",
    "            i += (chunk_size - overlap)\n",
    "    elif strategy == 'document':\n",
    "        chunks = df.apply(lambda row: row.to_dict(), axis=1).tolist()\n",
    "    return chunks\n",
    "\n",
    "def apply_embedding(chunks: list, strategy: str) -> list:\n",
    "    embedded_chunks = []\n",
    "    for chunk in chunks:\n",
    "        embedded_chunks.append(chunk)\n",
    "    return embedded_chunks\n",
    "\n",
    "def store_in_chromadb(embedded_chunks: list) -> None:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
